<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="ReflectionFlow"/>
  <meta property="og:description" content="From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  
  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="diffusion, t2i, reflection, dit">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">From Reflection to Perfection: <br>Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <a href="https://le-zhuo.com/" style="color:#008AD7;font-weight:normal;">Le Zhuo</a><sup style="color:#6fbf73;">1,</sup><sup style="color:#a8d5ba;">4</sup></sup><sup>*</sup>,</span>
              <a href="https://liangbingzhao.github.io/" style="color:#008AD7;font-weight:normal;">Liangbing Zhao</a><sup style="color:#ed4b82;">2</sup></sup><sup>*</sup>,</span>
              <a href="https://sayak.dev/" style="color:#008AD7;font-weight:normal;">Sayak Paul</a><sup style="color:#ffac33;">3</sup>,</span>
              <a href="https://liaoyue.net" style="color:#008AD7;font-weight:normal;">Yue Liao</a><sup style="color:#6fbf73;">1</sup>,</span>
              <a href="https://zrrskywalker.github.io/" style="color:#008AD7;font-weight:normal;">Renrui Zhang</a><sup style="color:#6fbf73;">1</sup>,</span>
              <a href="https://synbol.github.io/" style="color:#008AD7;font-weight:normal;">Yi Xin</a><sup style="color:#a8d5ba;">4</sup>,</span>
              <br>
              <a href="https://gaopengcuhk.github.io/" style="color:#008AD7;font-weight:normal;">Peng Gao</a><sup style="color:#a8d5ba;">4</sup>,</span>
              <a href="https://cemse.kaust.edu.sa/profiles/mohamed-elhoseiny" style="color:#008AD7;font-weight:normal;">Mohamed Elhoseiny</a><sup style="color:#ed4b82;">2</sup><sup>â˜¨</sup>,</span>
              <a href="https://www.ee.cuhk.edu.hk/~hsli/" style="color:#008AD7;font-weight:normal;">Hongsheng Li</a><sup style="color:#6fbf73;">1</sup><sup>â˜¨</sup></span>
            </div>

            <div class="is-size-4 publication-authors">
              <span class="author-block"><sup style="color:#6fbf73;">1</sup>CUHK MMLAB</span>&nbsp;
              <span class="author-block"><sup style="color:#ed4b82;">2</sup>KAUST</span>
              <span class="author-block"><sup style="color:#ffac33;">3</sup>Hugging Face</span>
              <span class="author-block"><sup style="color:#a8d5ba;">4</sup>Shanghai AI Lab</span>
            </div>

            <div class="is-size-9 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>&nbsp;
              <span class="author-block"><sup>â˜¨</sup>Corresponding Author</span>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2504.16080" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Diffusion-CoT/ReflectionFlow" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                  <!-- HF link -->
                  <span class="link-block">
                    <a href="https://huggingface.co/collections/diffusion-cot/reflectionflow-release-6803e14352b1b13a16aeda44" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">ðŸ¤—</span>
                      <span>GenRef</span>
                    </a>
                  </span>       
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <figure>
          <img src="static/images/fig1.png" width="80%"/>
          <figcaption>Overall pipeline of <strong>ReflectionFlow</strong> framework with both qualitative and quantitative results of scaling compute at inference time.</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent text-to-image diffusion models achieve impressive visual quality through extensive scaling of training data and model parameters, yet they often struggle with complex scenes and fine-grained details. Inspired by the self-reflection capabilities emergent in large language models, we propose <strong>ReflectionFlow</strong>, an inference-time framework enabling diffusion models to iteratively reflect upon and refine their outputs. <strong>ReflectionFlow</strong> introduces three complementary inference-time scaling axes: (1) noise-level scaling to optimize latent initialization; (2) prompt-level scaling for precise semantic guidance, and most notably; (3) reflection-level scaling, which explicitly provides actionable reflections to iteratively assess and correct previous generations. To facilitate reflection-level scaling, we construct <strong>GenRef</strong>, a large-scale dataset comprising 1 million triplets, each containing a reflection, a flawed image, and an enhanced image. Leveraging this dataset, we efficiently perform reflection tuning on state-of-the-art diffusion transformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified framework. Experimental results show that <strong>ReflectionFlow</strong> significantly outperforms naive noise-level scaling methods, offering a scalable and compute-efficient solution toward higher-quality image synthesis on challenging tasks. All code, checkpoints, and datasets are available.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Data -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Reflection Dataset</h2>
        <div class="content has-text-justified">
          <p>
            We introduce <strong>GenRef</strong>, the first large-scale dataset tailored for text-to-image refinement tasks, comprising over <strong>1 million triplets</strong> structured as <em>(flawed image, refined image, textual reflection)</em>. Our dataset spans four diverse data domains:
          </p>
          
          <ol>
            <li><strong>Rule-based Data:</strong> Challenging object-centric prompts verified by rule-based methods, ensuring diverse, clearly-defined image errors. Images are generated, rigorously evaluated, and paired by difficulty to form refinement pairs.</li>
            
            <li><strong>Reward-based Data:</strong> Images generated from general-purpose prompts and scored using ensemble reward models (HPSv2, CLIP, PickScore). High-quality and lower-quality images are paired to enhance aesthetic and semantic alignment.</li>
            
            <li><strong>Long-Short Prompt Data:</strong> Pairs of images generated from detailed versus concise prompts, leveraging the insight that longer, descriptive prompts yield higher-quality images.</li>
            
            <li><strong>Editing Data:</strong> High-quality editing pairs from OmniEdit with precise, actionable textual editing instructions and corresponding refined images, further enriching dataset diversity.</li>
          </ol>
          
          <p>
            Additionally, we collected <strong>227K chain-of-thought reflection annotations</strong> using advanced LLMs like GPT-4o. These annotations detail image differences, label image preferences, and provide concise instructions for refinement. We fine-tuned a dedicated MLLM verifier (Qwen2.5-VL-7B) to generate accurate textual reflections at scale and trained an image reward model to rigorously evaluate image quality gaps.
          </p>
        </div>
        <img src="static/images/fig2.png" width="80%"/>
      </div>
    </div>
  </div>
</section>
<!-- End Data -->

<!-- TTS -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Inference-Time Scaling Framework</h2>
        <div class="content has-text-justified">
          <p>
            We first efficiently fine-tune a pretrained diffusion transformer as our corrector model using the GenRef dataset, employing multimodal attention and targeted training strategies to learn image refinement.
          </p>
          <p>
            We then introduce <strong>ReflectionFlow</strong>, a versatile inference-time scaling framework designed to maximize T2I diffusion model performance through iterative refinement along three complementary dimensions:
          </p>
          
          <ol>
            <li><strong>Noise-Level Scaling:</strong> Generate multiple candidate images from diverse initial noise samples to explore a broad range of image possibilities.</li>
            
            <li><strong>Reflection-Level Scaling:</strong> Iteratively refine images using our trained corrector model guided by explicit textual reflections, progressively improving image quality.</li>
            
            <li><strong>Prompt-Level Scaling:</strong> Dynamically evolve and enhance textual prompts via a multimodal verifier model, ensuring increasingly precise guidance for subsequent iterations.</li>
          </ol>
          
          <p>
            In each iterative round, ReflectionFlow leverages multimodal evaluations, textual reflections, and refined prompts to enhance generated image quality. The framework flexibly balances computational cost and performance by adjusting both the number of parallel image generations (<em>search width</em>) and iterative refinement rounds (<em>reflection depth</em>).
          </p>
        </div>
        <img src="static/images/fig3.jpg" width="80%"/>
      </div>
    </div>
  </div>
</section>
<!-- End TTS -->

<!-- Qualitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Main Results</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate our <strong>ReflectionFlow</strong> framework on the GenEval benchmark, progressively applying three scaling dimensions: noise-level, prompt-level, and reflection-level scaling. Starting from a baseline FLUX.1-dev model (0.67), incorporating noise-level scaling notably boosts performance to <strong>0.85</strong>. Adding prompt-level scaling further increases performance to <strong>0.87</strong>, while integrating reflection-level scaling via explicit textual reflections achieves a substantial improvement, reaching <strong>0.91</strong>.
          </p>
          <p>
            Our method significantly outperforms state-of-the-art text-to-image models and existing reflection-based approaches (e.g., Reflect-DiT), clearly demonstrating that iterative refinement guided by multimodal textual feedback is highly effective for enhancing image generation quality.
          </p>
        </div>
        <img src="static/images/exp1.png" width="80%"/>
      </div>
    </div>
  </div>
</section>
<!-- End Qualitative -->
  
<!-- Ablation -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Ablations</h2>
        <div class="content has-text-justified">
          <h3>Exploring Different Verifiers</h3>
          <p>
            We evaluate ReflectionFlow with three verifiers: GPT-4o, our fine-tuned verifier, and SANA. All verifiers demonstrate strong performance, with GPT-4o quickly reaching its ceiling, our fine-tuned verifier showing steady improvement with increased sampling, and SANA achieving the highest score (0.91) at 32 samples. Compared to the oracle upper bound (0.98), there remains significant room for improvement through enhanced verifiers.
          </p>
          
          <h3>Scaling Inference-Time Budgets</h3>
          <p>
            By varying reflection depth while maintaining search width, we find that ReflectionFlow rapidly improves generation quality as computational budget increases. Our approach consistently outperforms baseline methods (Noise Scaling and Prompt Scaling), reaching a GenEval score of 0.91 at 32 samples with potential for further gains at larger budgets.
          </p>
          
          <h3>Exploring Iterative Refinement Strategies</h3>
          <p>
            Our analysis of different refinement strategies reveals that deeper sequential refinement consistently outperforms wider parallel generation. This confirms ReflectionFlow effectively leverages iterative reasoning to progressively correct complex errors, with optimal results achieved through higher refinement depth.
          </p>
          
          <h3>Reflection Capability for Difficult Tasks</h3>
          <p>
            ReflectionFlow shows remarkable effectiveness on challenging tasks. When stratifying prompts by difficulty, we observe the most significant improvements on hard prompts (correctness from 0.10 to 0.81), moderate gains on medium prompts (0.55 to 0.85), and minimal changes on easy prompts (0.95 to 0.97). This suggests potential for dynamically allocating resources based on task difficulty.
          </p>
        </div>
        <img src="static/images/exp2.png" width="100%"/>
      </div>
    </div>
  </div>
</section>
<!-- End Ablation -->

<!-- Complex reasoning -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Step by Step Complex Reasoning Results</h2>
        <img src="static/images/complexreasoning.jpg" width="80%"/>
        <div class="content has-text-justified">
          <p>
            Qualitative examples demonstrate that ReflectionFlow iteratively identifies and corrects detailed errors from initial image generations, progressively refining outputs to match complex prompts. This explicit, step-by-step refinement resembles interpretable chain-of-thought reasoning in language models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Complex Reasoning -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{zhuo2025reflectionperfectionscalinginferencetime,
      title={From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning}, 
      author={Le Zhuo and Liangbing Zhao and Sayak Paul and Yue Liao and Renrui Zhang and Yi Xin and Peng Gao and Mohamed Elhoseiny and Hongsheng Li},
      year={2025},
      eprint={2504.16080},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.16080}, <!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="ReflectionFlow"/>
  <meta property="og:description" content="From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  
  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="diffusion, t2i, reflection, dit">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">From Reflection to Perfection: <br>Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <a href="https://le-zhuo.com/" style="color:#008AD7;font-weight:normal;">Le Zhuo</a><sup style="color:#6fbf73;">1,</sup><sup style="color:#a8d5ba;">4</sup></sup><sup>*</sup>,</span>
              <a href="https://liangbingzhao.github.io/" style="color:#008AD7;font-weight:normal;">Liangbing Zhao</a><sup style="color:#ed4b82;">2</sup></sup><sup>*</sup>,</span>
              <a href="https://sayak.dev/" style="color:#008AD7;font-weight:normal;">Sayak Paul</a><sup style="color:#ffac33;">3</sup>,</span>
              <a href="https://liaoyue.net" style="color:#008AD7;font-weight:normal;">Yue Liao</a><sup style="color:#6fbf73;">1</sup>,</span>
              <a href="https://zrrskywalker.github.io/" style="color:#008AD7;font-weight:normal;">Renrui Zhang</a><sup style="color:#6fbf73;">1</sup>,</span>
              <a href="https://synbol.github.io/" style="color:#008AD7;font-weight:normal;">Yi Xin</a><sup style="color:#a8d5ba;">4</sup>,</span>
              <br>
              <a href="https://gaopengcuhk.github.io/" style="color:#008AD7;font-weight:normal;">Peng Gao</a><sup style="color:#a8d5ba;">4</sup>,</span>
              <a href="https://cemse.kaust.edu.sa/profiles/mohamed-elhoseiny" style="color:#008AD7;font-weight:normal;">Mohamed Elhoseiny</a><sup style="color:#ed4b82;">2</sup><sup>â˜¨</sup>,</span>
              <a href="https://www.ee.cuhk.edu.hk/~hsli/" style="color:#008AD7;font-weight:normal;">Hongsheng Li</a><sup style="color:#6fbf73;">1</sup><sup>â˜¨</sup></span>
            </div>

            <div class="is-size-4 publication-authors">
              <span class="author-block"><sup style="color:#6fbf73;">1</sup>CUHK MMLAB</span>&nbsp;
              <span class="author-block"><sup style="color:#ed4b82;">2</sup>KAUST</span>
              <span class="author-block"><sup style="color:#ffac33;">3</sup>Hugging Face</span>
              <span class="author-block"><sup style="color:#a8d5ba;">4</sup>Shanghai AI Lab</span>
            </div>

            <div class="is-size-9 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>&nbsp;
              <span class="author-block"><sup>â˜¨</sup>Corresponding Author</span>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2504.16080" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Diffusion-CoT/ReflectionFlow" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                  <!-- HF link -->
                  <span class="link-block">
                    <a href="https://huggingface.co/collections/diffusion-cot/reflectionflow-release-6803e14352b1b13a16aeda44" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">ðŸ¤—</span>
                      <span>GenRef</span>
                    </a>
                  </span>       
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <figure>
          <img src="static/images/fig1.png" width="80%"/>
          <figcaption>Overall pipeline of <strong>ReflectionFlow</strong> framework with both qualitative and quantitative results of scaling compute at inference time.</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent text-to-image diffusion models achieve impressive visual quality through extensive scaling of training data and model parameters, yet they often struggle with complex scenes and fine-grained details. Inspired by the self-reflection capabilities emergent in large language models, we propose <strong>ReflectionFlow</strong>, an inference-time framework enabling diffusion models to iteratively reflect upon and refine their outputs. <strong>ReflectionFlow</strong> introduces three complementary inference-time scaling axes: (1) noise-level scaling to optimize latent initialization; (2) prompt-level scaling for precise semantic guidance, and most notably; (3) reflection-level scaling, which explicitly provides actionable reflections to iteratively assess and correct previous generations. To facilitate reflection-level scaling, we construct <strong>GenRef</strong>, a large-scale dataset comprising 1 million triplets, each containing a reflection, a flawed image, and an enhanced image. Leveraging this dataset, we efficiently perform reflection tuning on state-of-the-art diffusion transformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified framework. Experimental results show that <strong>ReflectionFlow</strong> significantly outperforms naive noise-level scaling methods, offering a scalable and compute-efficient solution toward higher-quality image synthesis on challenging tasks. All code, checkpoints, and datasets are available.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Data -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Reflection Dataset</h2>
        <div class="content has-text-justified">
          <p>
            We introduce <strong>GenRef</strong>, the first large-scale dataset tailored for text-to-image refinement tasks, comprising over <strong>1 million triplets</strong> structured as <em>(flawed image, refined image, textual reflection)</em>. Our dataset spans four diverse data domains:
          </p>
          
          <ol>
            <li><strong>Rule-based Data:</strong> Challenging object-centric prompts verified by rule-based methods, ensuring diverse, clearly-defined image errors. Images are generated, rigorously evaluated, and paired by difficulty to form refinement pairs.</li>
            
            <li><strong>Reward-based Data:</strong> Images generated from general-purpose prompts and scored using ensemble reward models (HPSv2, CLIP, PickScore). High-quality and lower-quality images are paired to enhance aesthetic and semantic alignment.</li>
            
            <li><strong>Long-Short Prompt Data:</strong> Pairs of images generated from detailed versus concise prompts, leveraging the insight that longer, descriptive prompts yield higher-quality images.</li>
            
            <li><strong>Editing Data:</strong> High-quality editing pairs from OmniEdit with precise, actionable textual editing instructions and corresponding refined images, further enriching dataset diversity.</li>
          </ol>
          
          <p>
            Additionally, we collected <strong>227K chain-of-thought reflection annotations</strong> using advanced LLMs like GPT-4o. These annotations detail image differences, label image preferences, and provide concise instructions for refinement. We fine-tuned a dedicated MLLM verifier (Qwen2.5-VL-7B) to generate accurate textual reflections at scale and trained an image reward model to rigorously evaluate image quality gaps.
          </p>
        </div>
        <img src="static/images/fig2.png" width="80%"/>
      </div>
    </div>
  </div>
</section>
<!-- End Data -->

<!-- TTS -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Inference-Time Scaling Framework</h2>
        <div class="content has-text-justified">
          <p>
            We first efficiently fine-tune a pretrained diffusion transformer as our corrector model using the GenRef dataset, employing multimodal attention and targeted training strategies to learn image refinement.
          </p>
          <p>
            We then introduce <strong>ReflectionFlow</strong>, a versatile inference-time scaling framework designed to maximize T2I diffusion model performance through iterative refinement along three complementary dimensions:
          </p>
          
          <ol>
            <li><strong>Noise-Level Scaling:</strong> Generate multiple candidate images from diverse initial noise samples to explore a broad range of image possibilities.</li>
            
            <li><strong>Reflection-Level Scaling:</strong> Iteratively refine images using our trained corrector model guided by explicit textual reflections, progressively improving image quality.</li>
            
            <li><strong>Prompt-Level Scaling:</strong> Dynamically evolve and enhance textual prompts via a multimodal verifier model, ensuring increasingly precise guidance for subsequent iterations.</li>
          </ol>
          
          <p>
            In each iterative round, ReflectionFlow leverages multimodal evaluations, textual reflections, and refined prompts to enhance generated image quality. The framework flexibly balances computational cost and performance by adjusting both the number of parallel image generations (<em>search width</em>) and iterative refinement rounds (<em>reflection depth</em>).
          </p>
        </div>
        <img src="static/images/fig3.jpg" width="80%"/>
      </div>
    </div>
  </div>
</section>
<!-- End TTS -->

<!-- Qualitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Main Results</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate our <strong>ReflectionFlow</strong> framework on the GenEval benchmark, progressively applying three scaling dimensions: noise-level, prompt-level, and reflection-level scaling. Starting from a baseline FLUX.1-dev model (0.67), incorporating noise-level scaling notably boosts performance to <strong>0.85</strong>. Adding prompt-level scaling further increases performance to <strong>0.87</strong>, while integrating reflection-level scaling via explicit textual reflections achieves a substantial improvement, reaching <strong>0.91</strong>.
          </p>
          <p>
            Our method significantly outperforms state-of-the-art text-to-image models and existing reflection-based approaches (e.g., Reflect-DiT), clearly demonstrating that iterative refinement guided by multimodal textual feedback is highly effective for enhancing image generation quality.
          </p>
        </div>
        <img src="static/images/exp1.png" width="80%"/>
      </div>
    </div>
  </div>
</section>
<!-- End Qualitative -->
  
<!-- Ablation -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Ablations</h2>
        <div class="content has-text-justified">
          <h3>Exploring Different Verifiers</h3>
          <p>
            We evaluate ReflectionFlow with three verifiers: GPT-4o, our fine-tuned verifier, and SANA. All verifiers demonstrate strong performance, with GPT-4o quickly reaching its ceiling, our fine-tuned verifier showing steady improvement with increased sampling, and SANA achieving the highest score (0.91) at 32 samples. Compared to the oracle upper bound (0.98), there remains significant room for improvement through enhanced verifiers.
          </p>
          
          <h3>Scaling Inference-Time Budgets</h3>
          <p>
            By varying reflection depth while maintaining search width, we find that ReflectionFlow rapidly improves generation quality as computational budget increases. Our approach consistently outperforms baseline methods (Noise Scaling and Prompt Scaling), reaching a GenEval score of 0.91 at 32 samples with potential for further gains at larger budgets.
          </p>
          
          <h3>Exploring Iterative Refinement Strategies</h3>
          <p>
            Our analysis of different refinement strategies reveals that deeper sequential refinement consistently outperforms wider parallel generation. This confirms ReflectionFlow effectively leverages iterative reasoning to progressively correct complex errors, with optimal results achieved through higher refinement depth.
          </p>
          
          <h3>Reflection Capability for Difficult Tasks</h3>
          <p>
            ReflectionFlow shows remarkable effectiveness on challenging tasks. When stratifying prompts by difficulty, we observe the most significant improvements on hard prompts (correctness from 0.10 to 0.81), moderate gains on medium prompts (0.55 to 0.85), and minimal changes on easy prompts (0.95 to 0.97). This suggests potential for dynamically allocating resources based on task difficulty.
          </p>
        </div>
        <img src="static/images/exp2.png" width="100%"/>
      </div>
    </div>
  </div>
</section>
<!-- End Ablation -->

<!-- Complex reasoning -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Step by Step Complex Reasoning Results</h2>
        <img src="static/images/complexreasoning.jpg" width="80%"/>
        <div class="content has-text-justified">
          <p>
            Qualitative examples demonstrate that ReflectionFlow iteratively identifies and corrects detailed errors from initial image generations, progressively refining outputs to match complex prompts. This explicit, step-by-step refinement resembles interpretable chain-of-thought reasoning in language models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Complex Reasoning -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{zhuo2025reflectionperfectionscalinginferencetime,
      title={From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning}, 
      author={Le Zhuo and Liangbing Zhao and Sayak Paul and Yue Liao and Renrui Zhang and Yi Xin and Peng Gao and Mohamed Elhoseiny and Hongsheng Li},
      year={2025},
      eprint={2504.16080},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.16080}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>

}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
